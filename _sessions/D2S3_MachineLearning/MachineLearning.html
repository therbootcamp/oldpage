<!DOCTYPE html>
<html>
  <head>
    <title>Machine Learning</title>
    <meta charset="utf-8">
    <meta name="author" content="The R Bootcamp Twitter: @therbootcamp" />
    <link href="libs/remark-css/example.css" rel="stylesheet" />
    <link rel="stylesheet" href="my-theme.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Machine Learning
### The R Bootcamp<br/>Twitter: <a href='https://twitter.com/therbootcamp'><span class="citation">@therbootcamp</span></a>
### September 2017

---






# What is machine learning?

.pull-left6[


### Algorithms autonomously learning from data.

Given data, an algorithm tunes its *parameters* to match the data, understand how it works, and make predictions for what will occur in the future.

&lt;img src="https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/mldiagram_A.png" width="80%" style="display: block; margin: auto;" /&gt;

]

.pull-right4[

&lt;img src="https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/machinelearningcartoon.png" width="70%" style="display: block; margin: auto;" /&gt;


]

---
# Everyone uses machine learning

.pull-left6[

### Everyone!

- How does Google know what search results you want?

- How does Amazon know what products to recommend?

- How does Netflix decide what shows you'll want to watch next?

- How do Tesla cars recognize objects and predict accidents?


&gt; Machine learning drives our algorithms for demand forecasting, product search ranking, product and deals recommendations, merchandising placements, fraud detection, translations, and much more. ~ Jeff Bezos, Amazon founder

]


.pull-right4[

&lt;img src="https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/mlexamples.png" width="100%" style="display: block; margin: auto;" /&gt;




]

---
# What is the basic machine learning process?

&lt;img src="https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/MLdiagram.png" width="95%" /&gt;






---
# Why do we separate training from prediction?

.pull-left4[

Just because an algorithm can fit past (training) data well, does *not* necessarily mean that it will *predict* new data well.


&lt;br&gt; 

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/stockpen.jpg" alt="Anyone can come up with a model of past stock performance. Predicting future performance is much more difficult." width="70%" /&gt;
&lt;p class="caption"&gt;Anyone can come up with a model of past stock performance. Predicting future performance is much more difficult.&lt;/p&gt;
&lt;/div&gt;



]

.pull-right6[


&gt; "Prediction is difficult, especially when it is about the future" ~ Niels Bohr

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/bohr.jpg" alt="Niels Bohr, Nobel Laureate in Physics" width="20%" /&gt;
&lt;p class="caption"&gt;Niels Bohr, Nobel Laureate in Physics&lt;/p&gt;
&lt;/div&gt;

&gt; "An economist is an expert who will know tomorrow why the things he predicted yesterday didn't happen today." ~ Evan Esar

&gt; "A prediction about the direction of the stock market tells you nothing about where stocks are headed, but a whole lot about the person doing the predicting" ~ Warren Buffett

]




---
# Training (fitting) vs. Testing (prediction)
&lt;img src="https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/fittingpredictiondarts_A.png" width="70%" style="display: block; margin: auto;" /&gt;

---
# Training (fitting) vs. Testing (prediction)
&lt;img src="https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/fittingpredictiondarts_B.png" width="70%" style="display: block; margin: auto;" /&gt;

---
# Training (fitting) vs. Testing (prediction)
&lt;img src="https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/fittingpredictiondarts_C.png" width="70%" style="display: block; margin: auto;" /&gt;

---
# Training (fitting) vs. Testing (prediction)
&lt;img src="https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/fittingpredictiondarts_D.png" width="70%" style="display: block; margin: auto;" /&gt;




---
# Why do we separate training from prediction?

- Data comes from two processes: *Signal* and *Noise* (aka Error).
&lt;br&gt;

&lt;img src="MachineLearning_files/figure-html/unnamed-chunk-11-1.png" width="80%" style="display: block; margin: auto;" /&gt;




---
# Why do we separate training from prediction?

- A good model is one that tries to capture the signal and ignore the noise
- A bad model is one that captures too much unpredictable noise,
    

&lt;img src="MachineLearning_files/figure-html/unnamed-chunk-12-1.png" width="80%" style="display: block; margin: auto;" /&gt;




---
# What machine learning algorithms are there?

.pull-left6[

- There are hundreds if not thousands of machine learning algorithms from many different fields.
    - E.g.; Computer vision, Natural language processing, reinforcement learning, graphical models


- In this section, we will focus on 4:

| Algorithm|Complexity?|
|:------|:----|
|     Regression| Low / Medium | 
|     Decision Trees| Low |
|     Random Forests| High |
|     Support Vector Machines| High |

]

.pull-right4[

Wikipedia lists 57 *Categories* of machine learning algorithms, each with dozens of examples

![](https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/wikipediaml.png)&lt;!-- --&gt;

]


---
# How do you fit and evaluate models in R?

.pull-left45[

### Fitting a model



```r
A_model &lt;- A_fun(formula = y ~.,
                 data = data_train,
                 ...)
```


| Argument| Description| Note |
|------:|:----|:---|
|     formula|  Formula indicating variables to use|  `y ~ .` is often used as a catch-all |
|     data|    The dataset for model training| |
|     ...|  Optional other arguments| See the function help page for details|


]

.pull-right5[


### Evaluating a model


```r
# Common ways to explore / use a model

A_model           # Print generic information

names(A_model)    # Show attributes

summary(A_model)  # Print summary information

predict(A_model,  # Predict test data
        newdata = data_test)  

plot(A_model)     # Visualize the model
```


]



---
# Regression with `glm()`

.pull-left5[

In regression, the criterion is modeled as the weighted sum of predictors times *weights* `\(\beta_{1}\)`, `\(\beta_{2}\)`

### Example: Default on a loan

One could model the risk of defaulting on a loan as:

`$$Risk = Age \times \beta_{age} + Income \times \beta_{income} + ...$$`

Training a model means finding values of `\(\beta_{Age}\)` and `\(\beta_{Income}\)` that 'best' match the training data.

&lt;img src="https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/regression.png" width="50%" style="display: block; margin: auto;" /&gt;


]


.pull-right5[

Create regressions using the `glm()` function (part of base-R)


```r
# glm() function for regression
glm(formula = y ~.,     # Formula
    data = data_train,  # Training data
    family, ...)        # Optional arguments

# Train glm model
loan_glm_model &lt;- glm(formula = risk ~ ., 
                      data = data_train)

# Predict new data with glm model
loan_glm_pred &lt;- predict(loan_glm_model,
                         newdata = data_test)
```



]


---
# Decision Trees with `rpart::rpart()`

.pull-left5[

In decision trees, the criterion is modeled as a sequence of logical Yes or No questions.

### Example: Default on a loan

![](https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/defaulttree.png)&lt;!-- --&gt;


]

.pull-right5[

Create decision trees using the `rpart` package


```r
# Load the rpart package
library(rpart)

# Calculating a decision tree in R
rpart(formula = y ~.,       # Formula 
      data = data_train,    # Training data
      method, parms, cost)  # Optional arguments

# Train rpart model
loan_rpart_model &lt;- rpart(formula = risk ~ ., 
                          data = loan_data,
                          method = "anova")

# Predict new data with rpart model
loan_rpart_pred &lt;- predict(loan_rpart_model,
                          newdata = data_test)
```


]

---
# Advanced algorithms

.pull-left5[

### Support Vector Machines with `e1071::svm()`


&lt;img src="https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/supportvectormachine.png" width="40%" style="display: block; margin: auto;" /&gt;


```r
# Creating support vector machine model
library(e1071)

svm_model &lt;- svm(formula = risk ~ .,
                 data = loan_data,
                 ...)
```



]


.pull-right5[

### Random Forests with `randomForest::randomForest()`


&lt;img src="https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/randomforest.png" width="50%" style="display: block; margin: auto;" /&gt;


```r
# Creating random forest model
library(randomForest)

rf_model &lt;- randomForest(formula = risk ~ .,
                         data = loan_data,
                         ...)
```

]




---
# How do I do machine learning in R?

.pull-left4[


If you're really into machine learning, packages such as `mlr` and `caret` can automate much of the the machine learning process.

&lt;img src="https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/mlrcaret.png" width="100%" style="display: block; margin: auto;" /&gt;



]



.pull-right6[

In the practical, we will go through the basic steps "by hand" so you can see the process:


```r
# Create training and test data
data_train &lt;- ...
data_test &lt;- ...

# Train models on training data
model_A &lt;- A_fun(formula = y ~ ., 
                 data = data_train)

# Model A predictions
pred_A &lt;- predict(model_A, 
                  newdata = data_test)

# Calculate Model A error
pred_err_A &lt;- mean(abs(pred_A - data_test$y))

# Compare to Models B, C, D...
```


]




---
# Questions?



---
# What is the history of machine learning?

- 1805 - 1809: Legendre and Gauss discover least squares. Soon after Galton defines **Regression** in a biological context, followed by Pearson for purely statistical analyses.

- 1952: Arthur Samuel creates first computer learning program for learning checkers and coins the term **Machine Learning** in 1959. 

- 1957: Frank Rosenblatt creates first **Neural Network** to simulate the thought process of the human brain.

- 1963: First algorithm for **Support Vector Machines** is developed by Vapnik &amp; Chervonenkis.

- 1967: **Nearest neighbor algorithm** is developed for classification

- 1984: Breiman &amp; Olshen publish the CART algorithm for **Decision Trees**, followed by Quinlan who publishes the ID3 algorithm followed by C4.5

- 1986: Rina Dechter introduces **Deep Learning**, with many subsequent updates in the 2000s.

- 1995: Tin Kam Ho develops first algorithm for **Random Forests**

Sources: Wikipedia, Bernard Marr, "A Short History of Machine Learning", Forbes.
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {window.dispatchEvent(new Event('resize'));});
(function() {var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler"); if (!r) return; s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }"; d.head.appendChild(s);})();</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
  }
});
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
